{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train = pd.read_csv('./ml5/train.csv', sep=';', na_values='None')\n",
    "test = pd.read_csv('./ml5/test.csv', sep=';', na_values='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   0  18393       2     168    62.0    110     80            1     1      0   \n",
       "1   1  20228       1     156    85.0    140     90            3     1      0   \n",
       "2   2  18857       1     165    64.0    130     70            3     1      0   \n",
       "3   3  17623       2     169    82.0    150    100            1     1      0   \n",
       "4   4  17474       1     156    56.0    100     60            1     1      0   \n",
       "\n",
       "   alco  active  cardio  \n",
       "0     0       1       0  \n",
       "1     0       1       1  \n",
       "2     0       0       1  \n",
       "3     0       1       1  \n",
       "4     0       0       0  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>26970.000000</td>\n",
       "      <td>26969.000000</td>\n",
       "      <td>27103.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50062.686900</td>\n",
       "      <td>19455.528400</td>\n",
       "      <td>1.353733</td>\n",
       "      <td>164.322267</td>\n",
       "      <td>74.120615</td>\n",
       "      <td>130.271300</td>\n",
       "      <td>95.689000</td>\n",
       "      <td>1.368033</td>\n",
       "      <td>1.222700</td>\n",
       "      <td>0.087838</td>\n",
       "      <td>0.054470</td>\n",
       "      <td>0.805926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28906.167148</td>\n",
       "      <td>2477.840922</td>\n",
       "      <td>0.478136</td>\n",
       "      <td>8.213740</td>\n",
       "      <td>14.341540</td>\n",
       "      <td>212.501152</td>\n",
       "      <td>163.852598</td>\n",
       "      <td>0.679166</td>\n",
       "      <td>0.566494</td>\n",
       "      <td>0.283065</td>\n",
       "      <td>0.226947</td>\n",
       "      <td>0.395494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>8865.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-130.000000</td>\n",
       "      <td>-90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24983.250000</td>\n",
       "      <td>17658.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>49995.500000</td>\n",
       "      <td>19694.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>75232.250000</td>\n",
       "      <td>21320.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99997.000000</td>\n",
       "      <td>23705.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>16020.000000</td>\n",
       "      <td>9100.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id           age        gender        height        weight  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean   50062.686900  19455.528400      1.353733    164.322267     74.120615   \n",
       "std    28906.167148   2477.840922      0.478136      8.213740     14.341540   \n",
       "min        5.000000   8865.000000      1.000000     50.000000     10.000000   \n",
       "25%    24983.250000  17658.000000      1.000000    159.000000     65.000000   \n",
       "50%    49995.500000  19694.000000      1.000000    165.000000     72.000000   \n",
       "75%    75232.250000  21320.000000      2.000000    170.000000     82.000000   \n",
       "max    99997.000000  23705.000000      2.000000    198.000000    183.000000   \n",
       "\n",
       "              ap_hi         ap_lo   cholesterol          gluc         smoke  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  26970.000000   \n",
       "mean     130.271300     95.689000      1.368033      1.222700      0.087838   \n",
       "std      212.501152    163.852598      0.679166      0.566494      0.283065   \n",
       "min     -130.000000    -90.000000      1.000000      1.000000      0.000000   \n",
       "25%      120.000000     80.000000      1.000000      1.000000      0.000000   \n",
       "50%      120.000000     80.000000      1.000000      1.000000      0.000000   \n",
       "75%      140.000000     90.000000      2.000000      1.000000      0.000000   \n",
       "max    16020.000000   9100.000000      3.000000      3.000000      1.000000   \n",
       "\n",
       "               alco        active  \n",
       "count  26969.000000  27103.000000  \n",
       "mean       0.054470      0.805926  \n",
       "std        0.226947      0.395494  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      1.000000  \n",
       "50%        0.000000      1.000000  \n",
       "75%        0.000000      1.000000  \n",
       "max        1.000000      1.000000  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "NONE_COLS = ['smoke', 'alco', 'active']\n",
    "NONE_FRACTION = 0.15\n",
    "np.random.seed(87342)\n",
    "for col in NONE_COLS:\n",
    "    choice = np.random.choice(len(train), int(NONE_FRACTION * len(train)))\n",
    "    train[col][choice] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([train.drop(['cardio'], 1), test], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "\n",
    "def memoize(func):\n",
    "    mem = {}\n",
    "    def memoizer(*args, **kwargs):\n",
    "        key = str(args) + str(kwargs)\n",
    "        if key not in mem:\n",
    "            mem[key] = func(*args, **kwargs)\n",
    "        return mem[key]\n",
    "    return memoizer\n",
    "\n",
    "@memoize    \n",
    "def levenshtein(s, t):\n",
    "    if s == '':\n",
    "        return len(t)\n",
    "    if t == '':\n",
    "        return len(s)\n",
    "    if s[-1] == t[-1]:\n",
    "        cost = 0\n",
    "    else:\n",
    "        cost = 1\n",
    "    \n",
    "    res = min([levenshtein(s[:-1], t)+1,\n",
    "               levenshtein(s, t[:-1])+1, \n",
    "               levenshtein(s[:-1], t[:-1]) + cost])\n",
    "    return res\n",
    "\n",
    "def fix_ap(hi, lo):\n",
    "    hi = [e // 10 if e >= 1000 and e % 100 == 0 else e for e in map(abs, hi)]\n",
    "    lo = [e // 10 if e >= 1000 and e % 100 == 0 else e for e in map(abs, lo)]\n",
    "    \n",
    "    pairs = Counter(zip(hi, lo)).most_common()\n",
    "    counter, dict_size = 0, 0\n",
    "    reference = []\n",
    "    while counter < 0.99 * len(hi) and dict_size < len(hi):\n",
    "        if pairs[dict_size][0][0] > pairs[dict_size][0][1]:\n",
    "            counter += pairs[dict_size][1]\n",
    "            hi_lo = pairs[dict_size][0]\n",
    "            reference.append((hi_lo, '%s:%s' % hi_lo))\n",
    "        dict_size += 1\n",
    "    \n",
    "    print(dict_size, counter / len(hi))\n",
    "    print(reference)\n",
    "\n",
    "    map_hi_lo = dict((hi_lo, hi_lo) for hi_lo, _ in reference)\n",
    "    \n",
    "    for hi_lo, _ in pairs:\n",
    "        match = min((levenshtein(r[1], '%s:%s' % hi_lo), r[0]) for r in reference)\n",
    "        map_hi_lo[hi_lo] = match[1]\n",
    "        \n",
    "    result = [map_hi_lo[hi_lo] for hi_lo in zip(hi, lo)]\n",
    "    return list(map(itemgetter(0), result)), list(map(itemgetter(1), result))\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917 0.99\n",
      "[((120, 80), '120:80'), ((140, 90), '140:90'), ((110, 70), '110:70'), ((130, 80), '130:80'), ((130, 90), '130:90'), ((140, 80), '140:80'), ((150, 90), '150:90'), ((110, 80), '110:80'), ((120, 70), '120:70'), ((160, 100), '160:100'), ((150, 100), '150:100'), ((100, 70), '100:70'), ((160, 90), '160:90'), ((140, 100), '140:100'), ((120, 90), '120:90'), ((100, 60), '100:60'), ((90, 60), '90:60'), ((150, 80), '150:80'), ((130, 70), '130:70'), ((120, 60), '120:60'), ((110, 60), '110:60'), ((100, 80), '100:80'), ((180, 100), '180:100'), ((160, 80), '160:80'), ((170, 100), '170:100'), ((110, 90), '110:90'), ((120, 79), '120:79'), ((125, 80), '125:80'), ((130, 100), '130:100'), ((140, 70), '140:70'), ((170, 90), '170:90'), ((160, 110), '160:110'), ((90, 70), '90:70'), ((180, 90), '180:90'), ((145, 90), '145:90'), ((180, 110), '180:110'), ((130, 85), '130:85'), ((150, 110), '150:110'), ((170, 110), '170:110'), ((150, 70), '150:70'), ((135, 80), '135:80'), ((115, 70), '115:70'), ((115, 80), '115:80'), ((130, 60), '130:60'), ((170, 80), '170:80'), ((135, 90), '135:90'), ((190, 100), '190:100'), ((120, 75), '120:75'), ((180, 120), '180:120'), ((120, 100), '120:100'), ((120, 85), '120:85'), ((160, 70), '160:70'), ((125, 85), '125:85'), ((80, 60), '80:60'), ((100, 90), '100:90'), ((125, 90), '125:90'), ((125, 70), '125:70'), ((130, 79), '130:79'), ((200, 100), '200:100'), ((160, 120), '160:120'), ((140, 110), '140:110'), ((140, 60), '140:60'), ((110, 69), '110:69'), ((115, 75), '115:75'), ((145, 80), '145:80'), ((110, 75), '110:75'), ((140, 95), '140:95'), ((140, 85), '140:85'), ((130, 89), '130:89'), ((170, 120), '170:120'), ((145, 100), '145:100'), ((105, 70), '105:70'), ((140, 89), '140:89'), ((135, 85), '135:85'), ((180, 80), '180:80'), ((150, 89), '150:89'), ((110, 79), '110:79'), ((125, 75), '125:75'), ((150, 95), '150:95'), ((200, 120), '200:120'), ((200, 110), '200:110'), ((145, 95), '145:95'), ((155, 90), '155:90'), ((190, 110), '190:110'), ((165, 100), '165:100'), ((150, 60), '150:60'), ((100, 65), '100:65'), ((90, 50), '90:50'), ((128, 80), '128:80'), ((160, 95), '160:95'), ((190, 90), '190:90'), ((170, 70), '170:70'), ((140, 79), '140:79'), ((120, 82), '120:82'), ((95, 60), '95:60'), ((190, 120), '190:120'), ((145, 85), '145:85'), ((155, 100), '155:100'), ((90, 80), '90:80'), ((100, 75), '100:75'), ((120, 69), '120:69'), ((100, 69), '100:69'), ((105, 80), '105:80'), ((130, 110), '130:110'), ((130, 95), '130:95'), ((110, 65), '110:65'), ((135, 95), '135:95'), ((160, 60), '160:60'), ((100, 50), '100:50'), ((130, 75), '130:75'), ((120, 65), '120:65'), ((141, 90), '141:90'), ((150, 85), '150:85'), ((135, 100), '135:100'), ((165, 90), '165:90'), ((120, 89), '120:89'), ((122, 80), '122:80'), ((155, 95), '155:95'), ((110, 85), '110:85'), ((80, 50), '80:50'), ((210, 110), '210:110'), ((190, 80), '190:80'), ((160, 89), '160:89'), ((85, 60), '85:60'), ((130, 69), '130:69'), ((105, 65), '105:65'), ((105, 60), '105:60'), ((200, 90), '200:90'), ((95, 65), '95:65'), ((140, 120), '140:120'), ((117, 70), '117:70'), ((220, 120), '220:120'), ((95, 70), '95:70'), ((190, 60), '190:60'), ((90, 65), '90:65'), ((121, 80), '121:80'), ((140, 75), '140:75'), ((200, 140), '200:140'), ((120, 81), '120:81'), ((180, 70), '180:70'), ((112, 80), '112:80'), ((150, 79), '150:79'), ((110, 59), '110:59'), ((148, 90), '148:90'), ((210, 120), '210:120'), ((120, 20), '120:20'), ((180, 130), '180:130'), ((150, 99), '150:99'), ((123, 80), '123:80'), ((100, 59), '100:59'), ((115, 60), '115:60'), ((155, 80), '155:80'), ((110, 50), '110:50'), ((110, 20), '110:20'), ((160, 105), '160:105'), ((200, 130), '200:130'), ((120, 78), '120:78'), ((150, 120), '150:120'), ((125, 95), '125:95'), ((115, 65), '115:65'), ((120, 30), '120:30'), ((160, 85), '160:85'), ((90, 59), '90:59'), ((105, 75), '105:75'), ((220, 110), '220:110'), ((160, 99), '160:99'), ((170, 95), '170:95'), ((149, 90), '149:90'), ((135, 70), '135:70'), ((120, 95), '120:95'), ((140, 98), '140:98'), ((210, 100), '210:100'), ((130, 87), '130:87'), ((127, 80), '127:80'), ((132, 80), '132:80'), ((125, 84), '125:84'), ((135, 89), '135:89'), ((185, 100), '185:100'), ((145, 70), '145:70'), ((906, 0), '906:0'), ((125, 60), '125:60'), ((120, 77), '120:77'), ((130, 40), '130:40'), ((125, 100), '125:100'), ((165, 80), '165:80'), ((130, 82), '130:82'), ((115, 90), '115:90'), ((906, 60), '906:60'), ((123, 82), '123:82'), ((140, 91), '140:91'), ((150, 98), '150:98'), ((110, 100), '110:100'), ((110, 66), '110:66'), ((136, 85), '136:85'), ((150, 50), '150:50'), ((148, 80), '148:80'), ((140, 87), '140:87'), ((210, 140), '210:140'), ((220, 100), '220:100'), ((124, 80), '124:80'), ((144, 90), '144:90'), ((151, 100), '151:100'), ((120, 83), '120:83'), ((115, 78), '115:78'), ((125, 65), '125:65'), ((80, 70), '80:70'), ((120, 59), '120:59'), ((138, 80), '138:80'), ((155, 91), '155:91'), ((123, 83), '123:83'), ((180, 105), '180:105'), ((150, 88), '150:88'), ((120, 110), '120:110'), ((170, 60), '170:60'), ((115, 85), '115:85'), ((14020, 90), '14020:90'), ((120, 50), '120:50'), ((127, 90), '127:90'), ((139, 89), '139:89'), ((180, 140), '180:140'), ((131, 80), '131:80'), ((111, 70), '111:70'), ((147, 90), '147:90'), ((170, 89), '170:89'), ((140, 92), '140:92'), ((134, 90), '134:90'), ((110, 7), '110:7'), ((130, 65), '130:65'), ((109, 70), '109:70'), ((128, 82), '128:82'), ((134, 84), '134:84'), ((168, 100), '168:100'), ((123, 74), '123:74'), ((165, 95), '165:95'), ((159, 90), '159:90'), ((112, 72), '112:72'), ((140, 88), '140:88'), ((102, 66), '102:66'), ((124, 76), '124:76'), ((165, 110), '165:110'), ((125, 76), '125:76'), ((125, 79), '125:79'), ((119, 78), '119:78'), ((117, 73), '117:73'), ((200, 80), '200:80'), ((155, 110), '155:110'), ((108, 0), '108:0'), ((160, 79), '160:79'), ((110, 73), '110:73'), ((143, 90), '143:90'), ((126, 82), '126:82'), ((907, 70), '907:70'), ((135, 75), '135:75'), ((180, 60), '180:60'), ((119, 74), '119:74'), ((160, 98), '160:98'), ((100, 79), '100:79'), ((120, 0), '120:0'), ((125, 83), '125:83'), ((136, 86), '136:86'), ((129, 90), '129:90'), ((147, 97), '147:97'), ((140, 40), '140:40'), ((118, 75), '118:75'), ((135, 78), '135:78'), ((121, 70), '121:70'), ((180, 89), '180:89'), ((122, 85), '122:85'), ((115, 77), '115:77'), ((155, 85), '155:85'), ((145, 75), '145:75'), ((125, 86), '125:86'), ((136, 90), '136:90'), ((127, 87), '127:87'), ((152, 90), '152:90'), ((150, 105), '150:105'), ((120, 84), '120:84'), ((102, 80), '102:80'), ((170, 104), '170:104'), ((118, 80), '118:80'), ((136, 80), '136:80'), ((108, 73), '108:73'), ((138, 85), '138:85'), ((220, 140), '220:140'), ((120, 8), '120:8'), ((145, 110), '145:110'), ((220, 130), '220:130'), ((16, 10), '16:10'), ((155, 70), '155:70'), ((171, 100), '171:100'), ((116, 80), '116:80'), ((175, 100), '175:100'), ((162, 98), '162:98'), ((160, 115), '160:115'), ((150, 75), '150:75'), ((117, 80), '117:80'), ((130, 86), '130:86'), ((163, 100), '163:100'), ((134, 80), '134:80'), ((150, 87), '150:87'), ((109, 67), '109:67'), ((110, 89), '110:89'), ((14020, 80), '14020:80'), ((150, 97), '150:97'), ((170, 94), '170:94'), ((130, 59), '130:59'), ((170, 130), '170:130'), ((150, 10), '150:10'), ((141, 100), '141:100'), ((110, 72), '110:72'), ((142, 85), '142:85'), ((135, 83), '135:83'), ((138, 68), '138:68'), ((145, 105), '145:105'), ((124, 79), '124:79'), ((130, 88), '130:88'), ((165, 107), '165:107'), ((124, 70), '124:70'), ((153, 87), '153:87'), ((130, 83), '130:83'), ((126, 84), '126:84'), ((155, 94), '155:94'), ((124, 84), '124:84'), ((132, 92), '132:92'), ((105, 90), '105:90'), ((143, 98), '143:98'), ((100, 66), '100:66'), ((149, 74), '149:74'), ((108, 80), '108:80'), ((137, 86), '137:86'), ((128, 74), '128:74'), ((117, 69), '117:69'), ((135, 96), '135:96'), ((160, 75), '160:75'), ((113, 72), '113:72'), ((132, 85), '132:85'), ((147, 94), '147:94'), ((106, 72), '106:72'), ((1420, 80), '1420:80'), ((142, 86), '142:86'), ((113, 65), '113:65'), ((145, 60), '145:60'), ((107, 67), '107:67'), ((109, 73), '109:73'), ((135, 86), '135:86'), ((140, 99), '140:99'), ((120, 72), '120:72'), ((139, 79), '139:79'), ((130, 93), '130:93'), ((118, 74), '118:74'), ((140, 78), '140:78'), ((124, 75), '124:75'), ((170, 10), '170:10'), ((130, 84), '130:84'), ((154, 94), '154:94'), ((117, 71), '117:71'), ((156, 88), '156:88'), ((95, 69), '95:69'), ((160, 87), '160:87'), ((140, 69), '140:69'), ((115, 74), '115:74'), ((155, 93), '155:93'), ((155, 105), '155:105'), ((145, 83), '145:83'), ((185, 90), '185:90'), ((133, 96), '133:96'), ((190, 95), '190:95'), ((112, 76), '112:76'), ((126, 77), '126:77'), ((128, 78), '128:78'), ((230, 120), '230:120'), ((138, 83), '138:83'), ((119, 70), '119:70'), ((210, 150), '210:150'), ((140, 96), '140:96'), ((138, 90), '138:90'), ((140, 84), '140:84'), ((110, 55), '110:55'), ((129, 80), '129:80'), ((141, 80), '141:80'), ((116, 69), '116:69'), ((149, 97), '149:97'), ((122, 78), '122:78'), ((240, 130), '240:130'), ((160, 92), '160:92'), ((70, 40), '70:40'), ((140, 65), '140:65'), ((127, 86), '127:86'), ((121, 85), '121:85'), ((160, 40), '160:40'), ((118, 83), '118:83'), ((168, 90), '168:90'), ((150, 92), '150:92'), ((126, 78), '126:78'), ((179, 113), '179:113'), ((110, 67), '110:67'), ((164, 100), '164:100'), ((143, 80), '143:80'), ((112, 71), '112:71'), ((113, 79), '113:79'), ((139, 80), '139:80'), ((140, 101), '140:101'), ((128, 89), '128:89'), ((113, 64), '113:64'), ((175, 95), '175:95'), ((90, 57), '90:57'), ((120, 73), '120:73'), ((70, 50), '70:50'), ((145, 96), '145:96'), ((101, 70), '101:70'), ((126, 86), '126:86'), ((105, 62), '105:62'), ((110, 95), '110:95'), ((145, 89), '145:89'), ((120, 87), '120:87'), ((127, 85), '127:85'), ((240, 100), '240:100'), ((147, 83), '147:83'), ((145, 82), '145:82'), ((175, 85), '175:85'), ((103, 70), '103:70'), ((160, 58), '160:58'), ((140, 0), '140:0'), ((156, 99), '156:99'), ((173, 89), '173:89'), ((137, 89), '137:89'), ((130, 77), '130:77'), ((110, 71), '110:71'), ((142, 80), '142:80'), ((127, 73), '127:73'), ((128, 70), '128:70'), ((140, 94), '140:94'), ((130, 68), '130:68'), ((107, 70), '107:70'), ((144, 91), '144:91'), ((166, 100), '166:100'), ((110, 68), '110:68'), ((154, 80), '154:80'), ((154, 100), '154:100'), ((121, 74), '121:74'), ((190, 99), '190:99'), ((125, 78), '125:78'), ((115, 72), '115:72'), ((126, 90), '126:90'), ((153, 97), '153:97'), ((138, 0), '138:0'), ((130, 20), '130:20'), ((151, 92), '151:92'), ((107, 65), '107:65'), ((85, 70), '85:70'), ((100, 72), '100:72'), ((124, 86), '124:86'), ((117, 84), '117:84'), ((129, 79), '129:79'), ((114, 80), '114:80'), ((104, 70), '104:70'), ((175, 110), '175:110'), ((114, 90), '114:90'), ((111, 71), '111:71'), ((125, 89), '125:89'), ((126, 79), '126:79'), ((90, 67), '90:67'), ((139, 90), '139:90'), ((114, 75), '114:75'), ((131, 86), '131:86'), ((136, 87), '136:87'), ((119, 80), '119:80'), ((140, 50), '140:50'), ((151, 84), '151:84'), ((128, 90), '128:90'), ((141, 82), '141:82'), ((148, 89), '148:89'), ((140, 81), '140:81'), ((130, 91), '130:91'), ((144, 95), '144:95'), ((99, 71), '99:71'), ((240, 110), '240:110'), ((146, 84), '146:84'), ((106, 70), '106:70'), ((150, 40), '150:40'), ((117, 78), '117:78'), ((118, 90), '118:90'), ((119, 72), '119:72'), ((177, 95), '177:95'), ((140, 10), '140:10'), ((125, 67), '125:67'), ((137, 87), '137:87'), ((105, 69), '105:69'), ((131, 90), '131:90'), ((133, 80), '133:80'), ((127, 72), '127:72'), ((147, 100), '147:100'), ((134, 89), '134:89'), ((170, 115), '170:115'), ((100, 85), '100:85'), ((140, 86), '140:86'), ((128, 85), '128:85'), ((116, 78), '116:78'), ((146, 89), '146:89'), ((123, 79), '123:79'), ((159, 100), '159:100'), ((148, 92), '148:92'), ((145, 92), '145:92'), ((146, 80), '146:80'), ((120, 40), '120:40'), ((13010, 80), '13010:80'), ((160, 140), '160:140'), ((106, 69), '106:69'), ((11020, 80), '11020:80'), ((125, 88), '125:88'), ((151, 65), '151:65'), ((141, 91), '141:91'), ((160, 55), '160:55'), ((137, 91), '137:91'), ((180, 95), '180:95'), ((143, 85), '143:85'), ((148, 0), '148:0'), ((152, 67), '152:67'), ((131, 89), '131:89'), ((180, 99), '180:99'), ((148, 84), '148:84'), ((185, 110), '185:110'), ((125, 81), '125:81'), ((130, 50), '130:50'), ((103, 65), '103:65'), ((164, 68), '164:68'), ((137, 92), '137:92'), ((128, 68), '128:68'), ((152, 74), '152:74'), ((120, 9), '120:9'), ((114, 67), '114:67'), ((141, 88), '141:88'), ((168, 74), '168:74'), ((143, 102), '143:102'), ((170, 64), '170:64'), ((140, 103), '140:103'), ((188, 105), '188:105'), ((213, 95), '213:95'), ((161, 73), '161:73'), ((190, 115), '190:115'), ((122, 70), '122:70'), ((16020, 80), '16020:80'), ((143, 91), '143:91'), ((113, 70), '113:70'), ((123, 45), '123:45'), ((141, 81), '141:81'), ((155, 75), '155:75'), ((107, 68), '107:68'), ((185, 120), '185:120'), ((95, 80), '95:80'), ((96, 69), '96:69'), ((168, 92), '168:92'), ((121, 67), '121:67'), ((165, 98), '165:98'), ((142, 92), '142:92'), ((162, 94), '162:94'), ((191, 122), '191:122'), ((140, 82), '140:82'), ((132, 70), '132:70'), ((91, 60), '91:60'), ((148, 97), '148:97'), ((152, 100), '152:100'), ((151, 90), '151:90'), ((118, 73), '118:73'), ((100, 20), '100:20'), ((151, 101), '151:101'), ((145, 102), '145:102'), ((160, 83), '160:83'), ((109, 69), '109:69'), ((99, 70), '99:70'), ((163, 93), '163:93'), ((152, 95), '152:95'), ((101, 68), '101:68'), ((155, 89), '155:89'), ((95, 49), '95:49'), ((116, 74), '116:74'), ((112, 90), '112:90'), ((128, 20), '128:20'), ((200, 180), '200:180'), ((140, 64), '140:64'), ((125, 73), '125:73'), ((153, 88), '153:88'), ((101, 77), '101:77'), ((102, 85), '102:85'), ((115, 79), '115:79'), ((166, 76), '166:76'), ((116, 81), '116:81'), ((99, 59), '99:59'), ((112, 81), '112:81'), ((134, 70), '134:70'), ((197, 100), '197:100'), ((154, 104), '154:104'), ((155, 106), '155:106'), ((103, 63), '103:63'), ((133, 83), '133:83'), ((160, 97), '160:97'), ((98, 80), '98:80'), ((1150, 90), '1150:90'), ((123, 88), '123:88'), ((158, 116), '158:116'), ((165, 75), '165:75'), ((172, 102), '172:102'), ((165, 65), '165:65'), ((115, 84), '115:84'), ((126, 70), '126:70'), ((155, 87), '155:87'), ((103, 64), '103:64'), ((153, 109), '153:109'), ((119, 64), '119:64'), ((114, 82), '114:82'), ((155, 99), '155:99'), ((170, 99), '170:99'), ((1110, 80), '1110:80'), ((123, 81), '123:81'), ((116, 63), '116:63'), ((1409, 90), '1409:90'), ((126, 73), '126:73'), ((127, 69), '127:69'), ((166, 126), '166:126'), ((112, 79), '112:79'), ((175, 97), '175:97'), ((118, 85), '118:85'), ((80, 69), '80:69'), ((104, 75), '104:75'), ((168, 91), '168:91'), ((145, 66), '145:66'), ((91, 56), '91:56'), ((147, 89), '147:89'), ((133, 86), '133:86'), ((123, 86), '123:86'), ((156, 96), '156:96'), ((180, 103), '180:103'), ((1202, 80), '1202:80'), ((190, 69), '190:69'), ((100, 98), '100:98'), ((119, 86), '119:86'), ((126, 60), '126:60'), ((100, 55), '100:55'), ((175, 114), '175:114'), ((131, 93), '131:93'), ((107, 74), '107:74'), ((133, 95), '133:95'), ((117, 76), '117:76'), ((109, 80), '109:80'), ((907, 0), '907:0'), ((174, 76), '174:76'), ((188, 110), '188:110'), ((131, 85), '131:85'), ((127, 75), '127:75'), ((138, 94), '138:94'), ((126, 85), '126:85'), ((128, 88), '128:88'), ((160, 30), '160:30'), ((141, 84), '141:84'), ((100, 62), '100:62'), ((105, 77), '105:77'), ((193, 90), '193:90'), ((148, 95), '148:95'), ((158, 80), '158:80'), ((150, 93), '150:93'), ((111, 80), '111:80'), ((90, 56), '90:56'), ((136, 67), '136:67'), ((122, 66), '122:66'), ((166, 99), '166:99'), ((153, 73), '153:73'), ((154, 87), '154:87'), ((155, 79), '155:79'), ((105, 73), '105:73'), ((180, 125), '180:125'), ((147, 108), '147:108'), ((134, 81), '134:81'), ((158, 91), '158:91'), ((174, 94), '174:94'), ((136, 74), '136:74'), ((90, 76), '90:76'), ((130, 0), '130:0'), ((909, 60), '909:60'), ((150, 103), '150:103'), ((162, 90), '162:90'), ((128, 86), '128:86'), ((153, 82), '153:82'), ((119, 61), '119:61'), ((141, 70), '141:70'), ((128, 98), '128:98'), ((150, 130), '150:130'), ((127, 50), '127:50'), ((154, 98), '154:98'), ((175, 93), '175:93'), ((196, 182), '196:182'), ((1130, 90), '1130:90'), ((120, 86), '120:86'), ((170, 98), '170:98'), ((122, 84), '122:84'), ((162, 85), '162:85'), ((124, 66), '124:66'), ((240, 80), '240:80'), ((170, 150), '170:150'), ((103, 78), '103:78'), ((170, 106), '170:106'), ((153, 103), '153:103'), ((176, 68), '176:68'), ((175, 90), '175:90'), ((121, 81), '121:81'), ((147, 70), '147:70'), ((170, 109), '170:109'), ((165, 70), '165:70'), ((158, 113), '158:113'), ((91, 62), '91:62'), ((124, 89), '124:89'), ((125, 69), '125:69'), ((164, 74), '164:74'), ((127, 83), '127:83'), ((171, 90), '171:90'), ((153, 104), '153:104'), ((119, 71), '119:71'), ((95, 75), '95:75'), ((100, 6), '100:6'), ((105, 85), '105:85'), ((132, 100), '132:100'), ((160, 101), '160:101'), ((195, 100), '195:100'), ((126, 74), '126:74'), ((157, 81), '157:81'), ((118, 86), '118:86'), ((129, 77), '129:77'), ((150, 112), '150:112'), ((155, 103), '155:103'), ((131, 83), '131:83'), ((136, 102), '136:102'), ((147, 84), '147:84'), ((148, 78), '148:78'), ((107, 80), '107:80'), ((138, 100), '138:100'), ((123, 85), '123:85'), ((180, 20), '180:20'), ((109, 90), '109:90'), ((111, 65), '111:65'), ((116, 60), '116:60'), ((190, 70), '190:70'), ((113, 75), '113:75'), ((114, 73), '114:73'), ((141, 94), '141:94'), ((96, 56), '96:56'), ((105, 71), '105:71'), ((148, 85), '148:85'), ((106, 77), '106:77'), ((133, 82), '133:82'), ((134, 96), '134:96'), ((509, 0), '509:0'), ((17, 12), '17:12'), ((172, 80), '172:80'), ((120, 62), '120:62'), ((113, 76), '113:76'), ((119, 82), '119:82'), ((132, 81), '132:81'), ((165, 52), '165:52'), ((107, 78), '107:78'), ((123, 99), '123:99'), ((150, 94), '150:94'), ((108, 66), '108:66'), ((109, 76), '109:76'), ((144, 99), '144:99'), ((165, 120), '165:120'), ((130, 10), '130:10'), ((142, 90), '142:90'), ((128, 76), '128:76'), ((171, 115), '171:115'), ((168, 99), '168:99'), ((130, 94), '130:94'), ((142, 102), '142:102'), ((142, 79), '142:79'), ((132, 88), '132:88'), ((100, 64), '100:64'), ((200, 160), '200:160'), ((150, 81), '150:81'), ((163, 107), '163:107'), ((135, 82), '135:82'), ((136, 71), '136:71'), ((161, 65), '161:65'), ((198, 78), '198:78'), ((167, 111), '167:111'), ((171, 104), '171:104'), ((128, 83), '128:83'), ((124, 72), '124:72'), ((166, 83), '166:83'), ((199, 100), '199:100'), ((157, 73), '157:73'), ((118, 78), '118:78'), ((158, 95), '158:95'), ((104, 80), '104:80'), ((175, 80), '175:80'), ((150, 74), '150:74'), ((190, 98), '190:98'), ((122, 73), '122:73'), ((215, 110), '215:110'), ((95, 55), '95:55'), ((1620, 80), '1620:80'), ((115, 73), '115:73'), ((158, 87), '158:87'), ((121, 0), '121:0'), ((118, 65), '118:65'), ((159, 99), '159:99'), ((169, 80), '169:80'), ((168, 104), '168:104'), ((146, 82), '146:82'), ((167, 85), '167:85'), ((153, 95), '153:95'), ((156, 60), '156:60'), ((115, 91), '115:91')]\n"
     ]
    }
   ],
   "source": [
    "ap_hi, ap_lo = fix_ap(data['ap_hi'], data['ap_lo'])\n",
    "data['ap_hi'] = ap_hi\n",
    "data['ap_lo'] = ap_lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "MAX_DEPTH = 5\n",
    "SUBSAMPLE = 0.75\n",
    "MAX_FEATURES = 0.5\n",
    "LEAF = 8\n",
    "N_ESTIMATORS = 718\n",
    "\n",
    "def fill_none(data):\n",
    "    ignore_columns = ['id']\n",
    "    max_count = max(data.count())\n",
    "    col_count = list(zip(data.columns, data.count().get_values()))\n",
    "    none_columns = [col for col, count in col_count if count < max_count]\n",
    "    X = data.drop(none_columns + ignore_columns, 1).as_matrix()\n",
    "    print(col_count)\n",
    "    for col in none_columns:\n",
    "        y = data[col].as_matrix()\n",
    "        nan_idx = np.isnan(y)\n",
    "        gb = GradientBoostingClassifier(n_estimators=N_ESTIMATORS, \n",
    "                                 learning_rate=LEARNING_RATE,\n",
    "                                 max_depth=MAX_DEPTH, \n",
    "                                 subsample=SUBSAMPLE,\n",
    "                                 max_features=MAX_FEATURES,\n",
    "                                 min_samples_leaf=LEAF)\n",
    "        gb.fit(X[nan_idx == False], y[nan_idx == False])\n",
    "        print(col, log_loss(y[nan_idx == False], gb.predict_proba(X[nan_idx == False])))\n",
    "        data[col][nan_idx == True] = gb.predict_proba(X[nan_idx == True])\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 100000), ('age', 100000), ('gender', 100000), ('height', 100000), ('weight', 100000), ('ap_hi', 100000), ('ap_lo', 100000), ('cholesterol', 100000), ('gluc', 100000), ('smoke', 87214), ('alco', 87255), ('active', 87343)]\n",
      "smoke 0.231730338398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alco 0.185326864478\n",
      "active 0.483488069685\n"
     ]
    }
   ],
   "source": [
    "data = fill_none(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CATEGORY_COLS = ['gender', 'cholesterol', 'gluc']\n",
    "data = pd.get_dummies(data, columns=CATEGORY_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>gender_1</th>\n",
       "      <th>gender_2</th>\n",
       "      <th>cholesterol_1</th>\n",
       "      <th>cholesterol_2</th>\n",
       "      <th>cholesterol_3</th>\n",
       "      <th>gluc_1</th>\n",
       "      <th>gluc_2</th>\n",
       "      <th>gluc_3</th>\n",
       "      <th>height_to_weight</th>\n",
       "      <th>ap_diff</th>\n",
       "      <th>ap_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>49999.500000</td>\n",
       "      <td>19464.864590</td>\n",
       "      <td>164.348140</td>\n",
       "      <td>74.180167</td>\n",
       "      <td>128.835960</td>\n",
       "      <td>81.506500</td>\n",
       "      <td>0.140530</td>\n",
       "      <td>0.110670</td>\n",
       "      <td>0.766252</td>\n",
       "      <td>0.649180</td>\n",
       "      <td>0.350820</td>\n",
       "      <td>0.747740</td>\n",
       "      <td>0.137300</td>\n",
       "      <td>0.114960</td>\n",
       "      <td>0.850150</td>\n",
       "      <td>0.074370</td>\n",
       "      <td>0.075480</td>\n",
       "      <td>2.287897</td>\n",
       "      <td>47.32946</td>\n",
       "      <td>0.645285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28867.657797</td>\n",
       "      <td>2470.428376</td>\n",
       "      <td>8.211187</td>\n",
       "      <td>14.379494</td>\n",
       "      <td>153.535263</td>\n",
       "      <td>10.082018</td>\n",
       "      <td>0.334384</td>\n",
       "      <td>0.303762</td>\n",
       "      <td>0.399155</td>\n",
       "      <td>0.477229</td>\n",
       "      <td>0.477229</td>\n",
       "      <td>0.434312</td>\n",
       "      <td>0.344166</td>\n",
       "      <td>0.318975</td>\n",
       "      <td>0.356926</td>\n",
       "      <td>0.262373</td>\n",
       "      <td>0.264166</td>\n",
       "      <td>0.409294</td>\n",
       "      <td>153.06387</td>\n",
       "      <td>0.059419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8865.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316940</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24999.750000</td>\n",
       "      <td>17662.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.012987</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>49999.500000</td>\n",
       "      <td>19700.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.293333</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74999.250000</td>\n",
       "      <td>21324.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.538462</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99999.000000</td>\n",
       "      <td>23713.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>16020.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>15940.00000</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id            age         height         weight  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean    49999.500000   19464.864590     164.348140      74.180167   \n",
       "std     28867.657797    2470.428376       8.211187      14.379494   \n",
       "min         0.000000    8865.000000      50.000000      10.000000   \n",
       "25%     24999.750000   17662.000000     159.000000      65.000000   \n",
       "50%     49999.500000   19700.000000     165.000000      72.000000   \n",
       "75%     74999.250000   21324.000000     170.000000      82.000000   \n",
       "max     99999.000000   23713.000000     250.000000     200.000000   \n",
       "\n",
       "               ap_hi          ap_lo          smoke           alco  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean      128.835960      81.506500       0.140530       0.110670   \n",
       "std       153.535263      10.082018       0.334384       0.303762   \n",
       "min        16.000000       0.000000       0.000000       0.000000   \n",
       "25%       120.000000      80.000000       0.000000       0.000000   \n",
       "50%       120.000000      80.000000       0.000000       0.000000   \n",
       "75%       140.000000      90.000000       0.000000       0.000000   \n",
       "max     16020.000000     182.000000       1.000000       1.000000   \n",
       "\n",
       "              active       gender_1       gender_2  cholesterol_1  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.766252       0.649180       0.350820       0.747740   \n",
       "std         0.399155       0.477229       0.477229       0.434312   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.793578       0.000000       0.000000       0.000000   \n",
       "50%         1.000000       1.000000       0.000000       1.000000   \n",
       "75%         1.000000       1.000000       1.000000       1.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       cholesterol_2  cholesterol_3         gluc_1         gluc_2  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.137300       0.114960       0.850150       0.074370   \n",
       "std         0.344166       0.318975       0.356926       0.262373   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       1.000000       0.000000   \n",
       "50%         0.000000       0.000000       1.000000       0.000000   \n",
       "75%         0.000000       0.000000       1.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              gluc_3  height_to_weight       ap_diff       ap_ratio  \n",
       "count  100000.000000     100000.000000  100000.00000  100000.000000  \n",
       "mean        0.075480          2.287897      47.32946       0.645285  \n",
       "std         0.264166          0.409294     153.06387       0.059419  \n",
       "min         0.000000          0.316940       2.00000       0.000000  \n",
       "25%         0.000000          2.012987      40.00000       0.615385  \n",
       "50%         0.000000          2.293333      40.00000       0.666667  \n",
       "75%         0.000000          2.538462      50.00000       0.666667  \n",
       "max         1.000000         16.900000   15940.00000       0.980000  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['height_to_weight'] = data['height'] / data['weight']\n",
    "data['ap_diff'] = data['ap_hi'] - data['ap_lo']\n",
    "data['ap_ratio'] = data['ap_lo'] / data['ap_hi']\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data.merge(train.loc[:,['id','cardio']], on=['id'])\n",
    "test = data.merge(test.loc[:,['id']], on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('train_.tsv', '\\t')\n",
    "test.to_csv('test_.tsv', '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['cardio', 'id'], 1).as_matrix()\n",
    "y_train = train['cardio'].as_matrix()\n",
    "X_test = test.drop(['id'], 1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.559966606308\n",
      "0.567460559812\n",
      "0.562885493956\n",
      "0.558825198533\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 0.577329283214\n",
    "# 0.58009529054\n",
    "# 0.576594181237\n",
    "# 0.575518196677\n",
    "\n",
    "#0.560638833189\n",
    "#0.56690827324\n",
    "#0.565880554625\n",
    "#0.558810237243\n",
    "\n",
    "\n",
    "NUMERIC_COLS = ['age', 'height', 'weight', 'ap_hi', 'ap_lo', 'height_to_weight', 'ap_diff', 'ap_ratio']\n",
    "\n",
    "data.loc[:, NUMERIC_COLS] = (data.loc[:, NUMERIC_COLS] - data.loc[:, NUMERIC_COLS].mean()) \\\n",
    "                             / data.loc[:, NUMERIC_COLS].std()\n",
    "\n",
    "df_train_ = data.merge(train.loc[:,['id','cardio']], on=['id'])\n",
    "df_test_ = data.merge(test.loc[:,['id']], on=['id'])\n",
    "\n",
    "X_train_ = df_train_.drop(['cardio', 'id'], 1).as_matrix()\n",
    "X_test_ = df_test_.drop(['id'], 1).as_matrix()\n",
    "\n",
    "X_train_proba_logreg = np.zeros_like(y_train, dtype=np.float32)\n",
    "\n",
    "logreg = LogisticRegression(C=100)\n",
    "stratified = StratifiedKFold(n_splits=4, shuffle=True, random_state=13821)\n",
    "for train_index, test_index in stratified.split(X_train_, y_train):\n",
    "    logreg.fit(X_train_[train_index,:], y_train[train_index])\n",
    "    X_train_proba_logreg[test_index] = logreg.predict_proba(X_train_[test_index,:])[:,1]\n",
    "    print(log_loss(y_train[test_index], [max(1e-12, min(1.-1e-12, e)) for e in X_train_proba_logreg[test_index]]))\n",
    "\n",
    "logreg.fit(X_train_, y_train)\n",
    "X_test_proba_logreg = logreg.predict_proba(X_test_)[:,1]\n",
    "\n",
    "X_train_logreg = np.column_stack((X_train, X_train_proba_logreg))\n",
    "X_test_logreg = np.column_stack((X_test, X_test_proba_logreg))\n",
    "\n",
    "\n",
    "X_weight = np.zeros_like(y_train, dtype=np.float32)\n",
    "\n",
    "kfold = KFold(n_splits=4, shuffle=True, random_state=3419512)\n",
    "for train_index, test_index in kfold.split(X_train_, [0]*len(X_train)):\n",
    "    clf = LogisticRegression(C=100)\n",
    "    X = np.vstack([X_train_[train_index], X_test_])\n",
    "    y = [0] * len(X_train_[train_index]) + [1] * len(X_test_)\n",
    "    clf.fit(X, y)\n",
    "    X_weight[test_index] = clf.predict_proba(X_train_[test_index])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.537734633039\n",
      "0.54084066701\n",
      "0.544774464519\n",
      "0.539460073504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "LEARNING_RATE = 0.05\n",
    "MAX_DEPTH = 5\n",
    "SUBSAMPLE = 0.75\n",
    "MAX_FEATURES = 0.5\n",
    "LEAF = 8\n",
    "N_ESTIMATORS = 120\n",
    "\n",
    "# 0.537393660189\n",
    "# 0.540224990743\n",
    "# 0.54374520056\n",
    "# 0.537955373991\n",
    "\n",
    "# 0.536985924234\n",
    "# 0.539724269209\n",
    "# 0.54363649945\n",
    "# 0.538452420793\n",
    "\n",
    "# 0.539974945211\n",
    "# 0.543183537273\n",
    "# 0.547672279472\n",
    "# 0.54227076643\n",
    "\n",
    "X_train_proba_gb = np.zeros_like(y_train, dtype=np.float32)\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=N_ESTIMATORS, \n",
    "                                 learning_rate=LEARNING_RATE,\n",
    "                                 max_depth=MAX_DEPTH, \n",
    "                                 subsample=SUBSAMPLE,\n",
    "                                 max_features=MAX_FEATURES,\n",
    "                                 min_samples_leaf=LEAF)\n",
    "\n",
    "stratified = StratifiedKFold(n_splits=4, shuffle=True, random_state=92343)\n",
    "for train_index, test_index in stratified.split(X_train_logreg, y_train):\n",
    "    gb.fit(X_train_logreg[train_index,:], y_train[train_index], sample_weight=X_weight[train_index])\n",
    "    X_train_proba_gb[test_index] = gb.predict_proba(X_train_logreg[test_index,:])[:,1]\n",
    "    print(log_loss(y_train[test_index], X_train_proba_gb[test_index]))\n",
    "\n",
    "gb.fit(X_train_logreg, y_train, sample_weight=X_weight)\n",
    "X_test_proba_gb = gb.predict_proba(X_test_logreg)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.538315868728\n",
      "0.546390919952\n",
      "0.543279467355\n",
      "0.540995008597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "LEAF = 24\n",
    "N_ESTIMATORS = 195\n",
    "\n",
    "\n",
    "# 0.53721941118\n",
    "# 0.545754740511\n",
    "# 0.541656026029\n",
    "# 0.539694171679\n",
    "\n",
    "# 0.537595501441\n",
    "# 0.545116374224\n",
    "# 0.541802491798\n",
    "# 0.539919672545\n",
    "\n",
    "\n",
    "X_train_proba_rf = np.zeros_like(y_train, dtype=np.float32)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=N_ESTIMATORS,\n",
    "                            min_samples_leaf=LEAF)\n",
    "\n",
    "stratified = StratifiedKFold(n_splits=4, shuffle=True, random_state=18134)\n",
    "for train_index, test_index in stratified.split(X_train_logreg, y_train):\n",
    "    rf.fit(X_train_logreg[train_index,:], y_train[train_index], sample_weight=X_weight[train_index])\n",
    "    y_pred = rf.predict_proba(X_train_logreg[test_index,:])\n",
    "    X_train_proba_rf[test_index] = y_pred[:,1]\n",
    "    print(log_loss(y_train[test_index], X_train_proba_rf[test_index]))\n",
    "\n",
    "rf.fit(X_train_logreg, y_train, sample_weight=X_weight)\n",
    "X_test_proba_rf = rf.predict_proba(X_test_logreg)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169 0.536552234024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x110fc3898>,\n",
       " <matplotlib.lines.Line2D at 0x110fc3b38>]"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAH6BJREFUeJzt3XuQHeV95vHvb27nzEUjjdDFoCsECUkgQNKgCxJY4WZB\nWCAGO+CNjXeNSaUWm7Vd3oB3K67F2Qqb9TomFSqxjO2wsTFksY0V8BpjMEEGBBoJISSBQIBtSVwE\nuo3mfnv3j1+3z9FhRprRXHqmz/OpemvO6e5z5m215um3336720IIiIhIcShJugIiIjJyFPoiIkVE\noS8iUkQU+iIiRUShLyJSRBT6IiJFRKEvIlJEFPoiIkVEoS8iUkTKkq5AoUmTJoXZs2cnXQ0RkTFl\n06ZN74cQJh9vuVEX+rNnz6ahoSHpaoiIjClm9tv+LKfuHRGRIqLQFxEpIgp9EZEiotAXESkiCn0R\nkSKi0BcRKSIKfRGRIpKa0D/ybgvrrlzLSz/clnRVRERGrdSEfsfBZq565M848OMnk66KiMiolZrQ\nz4zPAhBa2xKuiYjI6JWa0M9OUOiLiBxPakK/LFtGNyXQ3p50VURERq3UhD5mtJHF2tXSFxHpS3pC\nH2g3hb6IyLGkKvQ7LKPQFxE5hnSFfkmWkg6FvohIX1IV+p2lWUo7FfoiIn1JV+iXZCnp1OgdEZG+\npCv0y7KUdqmlLyLSl1SFfldpljKFvohIn9IV+uVZyhX6IiJ9SlXo95RlKO9W6IuI9CVVod9dkaW8\nR6EvItKXVIV+T0WWCoW+iEifUhX6oSJLeY+GbIqI9CVVod+TyZIJaumLiPQlVaFPJksWhb6ISF9S\nFvoZMnTQ1dGTdE1EREaldIV+pT89q71R/foiIr1JVehbNgr9w+riERHpTapCv6RKoS8iciypCn2L\nQr/jiLp3RER606/QN7M1ZrbTzHaZ2W19LPNxM9thZtvN7L686d1mtiUq64aq4r0pifr0OxrV0hcR\n6U3Z8RYws1LgbuBSYA+w0czWhRB25C0zB7gdWBlCOGhmU/K+ojWEcO4Q17tXpdUe+p1HFPoiIr3p\nT0t/KbArhPBGCKEDuB+4umCZzwJ3hxAOAoQQ9g1tNfunrDoDKPRFRPrSn9CfBuzOe78nmpZvLjDX\nzJ42sw1mtiZvXtbMGqLp1wyyvscUt/S7mhT6IiK9OW73zgC+Zw6wGpgOPGVmC0MIh4BZIYS9ZnYa\n8ISZvRRCeD3/w2Z2M3AzwMyZM0+4EuXjPPS7mxX6IiK96U9Lfy8wI+/99Ghavj3AuhBCZwjhTeBV\nfCdACGFv9PMN4ElgUeEvCCGsDSHUhxDqJ0+ePOCViJXVRH36zRq9IyLSm/6E/kZgjpmdamYVwPVA\n4Sich/BWPmY2Ce/uecPM6swskzd9JbCDYZKZUAlAz5GW4foVIiJj2nG7d0IIXWZ2C/AoUAp8N4Sw\n3czuABpCCOuieZeZ2Q6gG/hyCGG/mZ0PfMvMevAdzJ35o36GWnZiFQDdTQp9EZHe9KtPP4TwM+Bn\nBdP+Mu91AL4YlfxlngEWDr6a/ROHflDoi4j0KlVX5FZN8tDvUeiLiPQqVaFfXut9+tbSnHBNRERG\np1SFPmY0UwUtaumLiPQmXaEPtJVUYW0KfRGR3qQu9FtLqiltU/eOiEhvUhf67aVVlLarpS8i0pvU\nhX5HWRVlHQp9EZHepC/0y6sp71D3johIb1IX+l3lVVR0qaUvItKb9IV+RqEvItKX1IV+d6aabLe6\nd0REepO60O/JVpHtUUtfRKQ3qQv9UFlFZVDoi4j0Jn2hX1VNNc10dYakqyIiMuqkLvSprqKEQOsh\nPT1LRKRQ6kK/pNpvr9zyvrp4REQKpS/0x1UD0LZfI3hERAqlMPS9pd92QC19EZFCqQv98loP/Y5D\nCn0RkULpC/0J3r3TfkDdOyIihVIX+pmTagDoONCUcE1EREaf1IV+5dRaALoONCZcExGR0Sd9oT9l\nHABdBxX6IiKFUhf6Nad4Sz8cPpJwTURERp/UhX52kvfph8Nq6YuIFEpd6Ft5Gc1UYU1q6YuIFEpd\n6AO0lIyjtFktfRGRQqkM/eayWkpb1NIXESmUytBvKx9HRZta+iIihVIZ+u0VtWTaFfoiIoVSGfod\nlbVkOtW9IyJSKJWh31U5jqoutfRFRAqlMvS7q2up7lZLX0SkUCpDn5pxjKORnp6kKyIiMrqkMvRD\nbS0ZOmjar+fkiojkS2Xol4z3m641v6MuHhGRfP0KfTNbY2Y7zWyXmd3WxzIfN7MdZrbdzO7Lm36j\nmb0WlRuHquLHUlbnN11reksnc0VE8pUdbwEzKwXuBi4F9gAbzWxdCGFH3jJzgNuBlSGEg2Y2JZo+\nEfgqUA8EYFP02YNDvyo55ZPGA9Dy1qHh/DUiImNOf1r6S4FdIYQ3QggdwP3A1QXLfBa4Ow7zEMK+\naPpHgMdCCAeieY8Ba4am6n2rnDYRgJa3hnXfIiIy5vQn9KcBu/Pe74mm5ZsLzDWzp81sg5mtGcBn\nh1z1DA/9jncU+iIi+Y7bvTOA75kDrAamA0+Z2cL+ftjMbgZuBpg5c+agKzNuZh0AXfsODPq7RETS\npD8t/b3AjLz306Np+fYA60IInSGEN4FX8Z1Afz5LCGFtCKE+hFA/efLkgdS/V7WzvaXfs18tfRGR\nfP0J/Y3AHDM71cwqgOuBdQXLPIS38jGzSXh3zxvAo8BlZlZnZnXAZdG0YVVaU0kbGeygWvoiIvmO\n270TQugys1vwsC4FvhtC2G5mdwANIYR15MJ9B9ANfDmEsB/AzL6G7zgA7gghDH8Sm9FYWkfpYYW+\niEg+CyEkXYej1NfXh4aGhkF/z+uVZ/L2hPmsevvBIaiViMjoZmabQgj1x1sulVfkArRkJ5JtUUtf\nRCRfakO/vaqOqnadyBURyZfa0O+smci4TrX0RUTypTb0u8dPZHzPQUbZKQsRkUSlNvSpq6OWIzQd\n7Ey6JiIio0ZqQ7906kkAHNilLh4RkVhqQz87YwoAh3a+m3BNRERGj9SGftVpUwFoel2hLyISS23o\nT5jrod/2u33HWVJEpHikNvTrzvDune631NIXEYmlNvQrJo+nnQpsn0JfRCSW2tDHjP1lUyk/oNAX\nEYmlN/SBxsqpZBsV+iIisVSHfnPNVMa1KPRFRGKpDv2OCVOo61Doi4jEUh36PVM+xOSwj5amnqSr\nIiIyKqQ69MtnTaOcLt7aorH6IiKQ8tCvmjcTgANbfpdwTURERodUh/6EhTMAaHp5d8I1EREZHVId\n+pOXeEu/83W19EVEIOWhn/lQHS1Whe1RS19EBFIe+pjxbmYm2fcV+iIikPbQBxprZzDhsLp3RESg\nCEK/bcpMprb9lh4N1RcRSX/oh9NPZyrv8varR5KuiohI4lIf+pVnzwXgrad2JVwTEZHkpT70J58/\nB4DGhlcTromISPJSH/ofWnU6AF07FPoiIqkP/ZLqSt4qm0nmtwp9EZHUhz7Avrq5THx/Z9LVEBFJ\nXFGEfsupZ3F62zZam7qTroqISKKKIvTLly2milZe/5la+yJS3Ioi9KeuWQTAe794IeGaiIgkqyhC\nf/ol82glS88mhb6IFLeiCP2SijLerDmbia8/n3RVREQSVRShD/D+mR9mwZHnaNrXknRVREQSUzSh\nX33VxWTo4JV7fp10VUREEtOv0DezNWa208x2mdltvcz/tJm9Z2ZbonJT3rzuvOnrhrLyAzHvplV0\nUE7TuseTqoKISOLKjreAmZUCdwOXAnuAjWa2LoSwo2DRB0IIt/TyFa0hhHMHX9XBqZ5SzZba5UzZ\n9kTSVRERSUx/WvpLgV0hhDdCCB3A/cDVw1ut4bH/nIuZ17yJI787mHRVREQS0Z/QnwbkP29wTzSt\n0LVmttXMHjSzGXnTs2bWYGYbzOyawVR2sMZ/7FJKCOz8xiNJVkNEJDFDdSL3X4HZIYSzgceAe/Pm\nzQoh1AOfAL5pZn9Q+GEzuznaMTS89957Q1SlDzrn5uX8tmQ2mfvvPf7CIiIp1J/Q3wvkt9ynR9N+\nL4SwP4TQHr29B1iSN29v9PMN4ElgUeEvCCGsDSHUhxDqJ0+ePKAVGIjyTAkvnfspznz3cVp2/GbY\nfo+IyGjVn9DfCMwxs1PNrAK4HjhqFI6ZnZz39irg5Wh6nZlloteTgJVA4QngETX59pvoppQ3P/e/\nk6yGiEgijjt6J4TQZWa3AI8CpcB3QwjbzewOoCGEsA74vJldBXQBB4BPRx+fD3zLzHrwHcydvYz6\nGVFLr53BQ3Wf5PJf3UPY/RfYjOlJVkdExpqeHmhrg9ZWaGnxMtDXXV3+Pd3d/rq7G8xg3jy4445h\nrb6FEIb1FwxUfX19aGhoGNbfcf+dv+Hq2+dz8IKrOeWp+4f1d4nICIiDOL8MJpSP9bq19cTqWFEB\nlZVeysuhpATKyqC01EtPD5x1Fjz44Al9vZltis6fHtNxW/pp9NEvzubuv/kKX1j/l/T8nysp+dSf\nJl0lkbGlpwc6O6GjI1f6eh8HZW+lvd1buPGy7e0+vakJjhyBQ4f8Z1eXl3i5uHXc0eEB39FxYuth\n5iFcVeUl/3V1NUye/MHpJ/K6stIDfhQYHbUYYRUVMPWbt/PkjY+z6qbPUjJzOqxenXS1RPonbtU2\nNuZan72Fbns7NDf7z+5uL52dvYdxe3vuM3GQNjUd/Tvi0tbmoTuUSkshk/E/zmwWxo3zMmECnHSS\nTy8r85LJ5FrHces5m/WSyeTe9xXC+e8zGQ/+IlKUoQ9w/Z+W8cffepCpGz7MGVf+O0r+6Xtw3XVJ\nV0vGip4eOHzYAzEOVPAQamuDgwe9HDrk4RmHbH4Ax63V/PeHDx9dCgM3/o6hEodkHLj5P8eNg1NO\n8RZvVVUuSONwrajwUl6ee134vrw897neSibj9Siy4E1S0YZ+SQnc9YNJrDn7MR7s+Shnfexj8PGP\nw223waIPjCqVpHR0eCDEh/Lt7R9slRb+7O72UI6XjUtb2wen5c9rbfWWcUuLT4tbxYWlvd0Duadn\n8OsXt1jjUlvrrdvx472FO2PG0SGZ311QU+Mlkzk6hPNLdbXPi/uOy8tzJZv1PwQpKkUb+gCzZ8P/\n+N4pLLpuPff+wde44f99E/uXf4GLLoIbb4TFi2HuXP/jkd6F4EF4vH7blhbvm41L3Gdb+D7/Z1OT\nf/dwiFuz+SX/8L+u7uiAjEscphMmwMSJHqpxYIOvazbrn49LTY1/Ju5OiJcvKVELV0ZcUY7eKXTP\nPfDZz8JHLzrEP1/4barWfhPeestnlpXBnDk+lOqkk7wlVlvrLbG41NT4H3Hc51hZefQhcXm5f1f8\nbz2Yn3E/q5l/d2urdyEcPJhrncYt17hlG7eCOztzQ8Xi0t3t3xu/DyF30qyryz/b1HR0K/nIkdwJ\nt9bWE2vxxt0HcampOfp1XKqrffn8Pt/8lm3hz7jv1yy3XNwdkf85tXAlZTR6ZwBuim4E/ed/PoH5\nu77M93/wRS6Y/Aq8+CLs2AHbt8POnR6uhw97F8BYUlJydCCWlORamvnFzEt5eW4HVl7u4VtXlwvN\n2trc68Iuh8ISn2Crqjo63OMdoYiMKIV+5Kab4Oyz4YYb4MMXlfLFL57JV75yJhM/0cvCXV3e2o5P\ntjU3507EdXXlhpzFIx06O3OH8YP9WV3toQv+/VVVfrRRV5cb/1vYso27HkSk6Kl7p8CRI/ClL8G3\nv+0N0s99Dr7wBZg0KbEqiYgcV3+7d9SxWWDcOFi7FrZuhcsvh7/+az/he+utsG1b0rUTERkchX4f\nFi6EBx7woL/mGvjHf/RpK1bAd77jvTciImONQv84FiyA738f9u6Fb3zDu/BvuglOPhk++Un48Y/H\n3nldESleCv1+mjTJ+/a3b4enn/bruH72M7j2Wr89xzXXwL33woEDSddURKRvOpE7CF1d8NRT8JOf\nwEMPwZ49PlBm9Wq44gq45BK/aZ6GhIvIcOvviVyF/hAJARoacjuAl1/26VOmwMUXe7nkEpg1K9l6\nikg6KfQTtns3PP64l1/+Et55x6effnpuB/CHf+gX+YqIDJZCfxQJwS/s/eUvfSfw5JN+PYCZ394n\n3gmsWuXXV4mIDJRCfxTr7ISNG3M7gWef9WmZDKxcmdsJLFmii2lFpH8U+mNIUxOsX5/rCnrxRZ8+\nYYJ3AV18sZ8cnj9fJ4VFpHe64doYUlPjV/9efrm/37cPnnjCdwKPPeYnh8Fvr7NihR8NrFwJ553n\nt94REekvtfRHuRDgjTf8SODpp73EI4PKyvx5L/FOYOVKv2hMRIqPundSbP9+Pw/wzDO+E3j+eb+Z\nJ8Cpp/r5gCuv9J2ARgeJFAeFfhHp6IAXXvAdwK9/7ecF4tFBCxf6qKBVq+CCC2D69KRrKyLDQaFf\nxNrbYcMGv1p4/Xo/KohvEDdrlu8ArrgC/uiP/Fb8IjL26URuEctk4MMf9gJ+u4itW/0oID4S+MEP\n/Hkr9fV+BHDBBd4dVFeXbN1FZHippV+Eenrguefgpz/1o4GGhtzDvc46y3cAy5fDOefo3kEiY4W6\nd6TfWlr8ZPD69V6eeSZ3u+jJk+HSS3Nl2rRk6yoivVP3jvRbVZVf/LV6tb/v6oJXX/UjgMce83Lf\nfT5vwQIP/8su8+6j6uqkai0iJ0ItfTmuEOCll+AXv/Cyfr0PES0t9esELrrIy6pV2gmIJEXdOzJs\nWlt9eOivfuU7gA0b/JxAeTksW+a3jbjoIn+dySRdW5HioNCXEdPc7DuBJ57wsmmTnyyurMydFF6x\nAs4/H2prk66tSDop9CUxhw75qKB4J7B9u+8ESkq8OygeHrp0KcyY4aOGRGRwFPoyajQ35y4We+op\nfx3fNmLKFA//887L/dStI0QGTqN3ZNSors49MhL8iuGtW32Y6MaN/vORR/yEMfj9g5Yu9W6hVavg\n3HP95nIiMnj6U5IRl8l4i/6883LTGhth8+bcjuCZZ+CBB3xeTY2fE4ivHF62TE8YEzlR6t6RUWvv\n3twFY+vXw7ZtfjSg20eIfNCQ9umb2RrgLqAUuCeEcGfB/E8D/wvYG036+xDCPdG8G4H/Fk3/qxDC\nvcf6XQp96cvBgz5KKN4J5N8+4swz4eyz/QKzs8/2Zw+XlyddY5GRM2Shb2alwKvApcAeYCNwQwhh\nR94ynwbqQwi3FHx2ItAA1AMB2AQsCSEc7Ov3KfSlv1pb/R5C8bUCL7wAb7/t8zIZv430/Pk+VHTl\nSj860JPGJK2G8kTuUmBXCOGN6IvvB64GdhzzU+4jwGMhhAPRZx8D1gA/7MdnRY6psvLo20f09PhT\nxrZs8XMDu3f764cf9vnxk8bOPz+3I9C9hKTY9Cf0pwG7897vAZb1sty1ZnYhflTwhRDC7j4+qz8z\nGRYlJXD66V6uuy43Pf9JY888A2vXwl13+bwZM3wHcMEFfi+hBQt0V1FJt6EavfOvwA9DCO1m9mfA\nvcBF/f2wmd0M3Awwc+bMIaqSiDvpJH985JVX+vvOTj8CeOYZ3xk8/XRupFBtrQ8XXbbMy5IlcMop\nydVdZKj1J/T3AjPy3k8nd8IWgBDC/ry39wB/k/fZ1QWffbLwF4QQ1gJrwfv0+1EnkRNWXp4bMnrr\nrT4i6M03c08Ze+45uPNO6O725U85xa8ZiB8+v2gRVFQkuw4iJ6o/J3LL8C6bi/EQ3wh8IoSwPW+Z\nk0MIb0ev/xj4ixDC8uhE7iZgcbToZvxE7oG+fp9O5Mpo0NzsJ4Y3b/adwLPP+o4BIJv1HcbKlX40\nsHSpjgYkeUN2IjeE0GVmtwCP4kM2vxtC2G5mdwANIYR1wOfN7CqgCzgAfDr67AEz+xq+owC441iB\nLzJaVFfnHigfe/tt7xJ6+mkvX/+6P3sAPPSXLs2V+no9f1hGJ12cJXKC2tpyI4Xi8tprPs/MTwqv\nWJErZ5yhk8QyfHTDNZEEHDzot5GIu4Q2bPBpABMm+LmB+FbTy5bpaECGjkJfZBTo6fFHTz77bK5s\n3+4njwuPBpYvh3nzdDQgJ0ahLzJKNTZ6V1C8Eyg8Gli2LLcj0NGA9JdCX2SMKDwa2LAhd3O5+Ggg\n7hJasUJHA9I7hb7IGDaQo4GlS32aFDeFvkiKHO9oYP78o0cK6Wig+Cj0RVLuWEcD48cffTSwZAlM\nmpRsfWV4KfRFikx8NLBhQ25HEB8NAMydC5dcAhde6Bed6Q6j6aLQFxEaG/26gU2b/KH0Tz7pt5gA\nf95AfAXx4sVwzjn+oHoZmxT6IvIBnZ3w4ot+G4n4KuJdu3LzZ87M3Yxu6VL/WVOTXH2l/4byISoi\nkhLx84Xr86Jh/37fEWzZ4kcFGzfCj37k80pKfMjokiW5cu65egLZWKaWvoh8wP79uZPEDQ3ePbRv\nn88rKfHRQkuWeLdQvCPQEUGy1L0jIkMmBHjrLQ///PLOOz7fzIeJLlni3ULLl/s5Aj13YOQo9EVk\n2PW2I4gfTp/N+g4gfh7x8uUaNjqcFPoiMuJCgD17fNho/EzizZtzzx2YNs27gs49148EFi+G007z\nIwUZHJ3IFZERZ+YPm58xAz72MZ/W0uLnBZ5/PnfC+Oc/zz2OctIkPxqIjwjq6/0oQYaHQl9EhlVV\nlV8QduGFuWltbX6L6YaG3FHBunU+r6LCjwQWLfJy3nmwcKGPPJLBU/eOiIwK773n4f/rX/vOYMsW\nOHTI52WzvgNYtiw3auiMM6C0NNk6jybq0xeRMS0E+M1vjn4c5aZN0Nrq86urPfwXL84dGcyfX7wj\nhtSnLyJjmhmceqqXP/kTn9bVBa+84ieHGxr8QrJvf9vPG4B3AZ111tEXky1cqHME+dTSF5Exrbvb\nbyXxwgu5smkTHDjg88vKclcVx1cjL1wIlZXJ1nuoqXtHRIpW3DW0eXNuJ9DQAO+/7/NLSnyo6Jln\nevdQfL+hsXwdgbp3RKRo5XcNXXutTwsBdu/2LqGtW3300LZtPmoobvvOnJm7tUR8wnjq1OTWYzgo\n9EWkKJh5qM+cmdsRgN9+evNm3xls3uxHBQ89lJs/bZqH/6JFuRPGs2aN3QvKFPoiUtRqa2H1ai+x\nxkbvFop3Aps3wyOP+INqwJ9JfM45vhNYssTvO7RggY8oGu3Upy8i0g8tLfDSS7mTxVu3eolHDpn5\ntQP53UPnnDNyD61Xn76IyBCqqvKLw5Yty03r7oadO/0xlS++6EcE//ZvcN99uWVmzcpdT7BokZ88\nTrJ7SKEvInKCSku9W2fBArjmmtz0d9/1HUB8r6EXXoCf/CQ3f/x4PwrIP08wUheWqXtHRGQENDbm\nRg29+GKuiyjuHspkfMdx//0n9v3q3hERGUVqa2HVKi+x7m547TU/Gnj++ZG5YEyhLyKSkNJSH/kz\nbx5cf/3I/M6Skfk1IiIyGij0RUSKiEJfRKSIKPRFRIqIQl9EpIgo9EVEiohCX0SkiCj0RUSKyKi7\nDYOZvQf8dhBfMQl4f4iqM1ZondOv2NYXtM4DNSuEMPl4C4260B8sM2voz/0n0kTrnH7Ftr6gdR4u\n6t4RESkiCn0RkSKSxtBfm3QFEqB1Tr9iW1/QOg+L1PXpi4hI39LY0hcRkT6kJvTNbI2Z7TSzXWZ2\nW9L1GSpmNsPMfmVmO8xsu5ndGk2faGaPmdlr0c+6aLqZ2d9F/w5bzWxxsmtw4sys1MxeMLOHo/en\nmtlz0bo9YGYV0fRM9H5XNH92kvU+UWY2wcweNLNXzOxlM1uR9u1sZl+I/l9vM7Mfmlk2bdvZzL5r\nZvvMbFvetAFvVzO7MVr+NTO78UTrk4rQN7NS4G7gcmABcIOZLUi2VkOmC/hSCGEBsBz4T9G63QY8\nHkKYAzwevQf/N5gTlZuBfxj5Kg+ZW4GX897/T+BvQwinAweBz0TTPwMcjKb/bbTcWHQX8PMQwjzg\nHHzdU7udzWwa8HmgPoRwFlAKXE/6tvM/AWsKpg1ou5rZROCrwDJgKfDVeEcxYCGEMV+AFcCjee9v\nB25Pul7DtK4/BS4FdgInR9NOBnZGr78F3JC3/O+XG0sFmB79MVwEPAwYftFKWeE2Bx4FVkSvy6Ll\nLOl1GOD6jgfeLKx3mrczMA3YDUyMttvDwEfSuJ2B2cC2E92uwA3At/KmH7XcQEoqWvrk/vPE9kTT\nUiU6nF0EPAdMDSG8Hc16B5gavU7Lv8U3gf8C9ETvTwIOhRC6ovf56/X7dY7mH46WH0tOBd4Dvhd1\nad1jZtWkeDuHEPYCXwd+B7yNb7dNpHs7xwa6XYdse6cl9FPPzGqAHwH/OYTQmD8v+K4/NcOwzOxK\nYF8IYVPSdRlBZcBi4B9CCIuAZnKH/EAqt3MdcDW+wzsFqOaD3SCpN9LbNS2hvxeYkfd+ejQtFcys\nHA/8H4QQfhxNftfMTo7mnwzsi6an4d9iJXCVmf0GuB/v4rkLmGBmZdEy+ev1+3WO5o8H9o9khYfA\nHmBPCOG56P2D+E4gzdv5EuDNEMJ7IYRO4Mf4tk/zdo4NdLsO2fZOS+hvBOZEZ/0r8JNB6xKu05Aw\nMwO+A7wcQvhG3qx1QHwG/0a8rz+e/qloFMBy4HDeYeSYEEK4PYQwPYQwG9+WT4QQ/j3wK+C6aLHC\ndY7/La6Llh9TLeIQwjvAbjM7I5p0MbCDFG9nvFtnuZlVRf/P43VO7XbOM9Dt+ihwmZnVRUdIl0XT\nBi7pExxDeKLkCuBV4HXgvyZdnyFcr1X4od9WYEtUrsD7Mh8HXgN+CUyMljd8JNPrwEv4yIjE12MQ\n678aeDh6fRrwPLAL+L9AJpqejd7viuaflnS9T3BdzwUaom39EFCX9u0M/HfgFWAb8M9AJm3bGfgh\nfs6iEz+i+8yJbFfgP0brvgv4DydaH12RKyJSRNLSvSMiIv2g0BcRKSIKfRGRIqLQFxEpIgp9EZEi\notAXESkiCn0RkSKi0BcRKSL/HxfrPEKe25cBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e094908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X_train_logreg, \n",
    "                                                        y_train,\n",
    "                                                        test_size=0.33, \n",
    "                                                        random_state=42)\n",
    "\n",
    "LEARNING_RATE = 0.05\n",
    "MAX_DEPTH = 5\n",
    "SUBSAMPLE = 0.75\n",
    "MAX_FEATURES = 0.5\n",
    "LEAF = 8 # 818 0.536890801542\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=1000, \n",
    "                                 learning_rate=LEARNING_RATE,\n",
    "                                 max_depth=MAX_DEPTH, \n",
    "                                 subsample=SUBSAMPLE,\n",
    "                                 max_features=MAX_FEATURES,\n",
    "                                 min_samples_leaf=LEAF)\n",
    "\n",
    "clf.fit(X_train_, y_train_)\n",
    "\n",
    "test_score = np.zeros((clf.n_estimators,), dtype=np.float64)\n",
    "for i, y_pred in enumerate(clf.staged_predict_proba(X_test_)):\n",
    "    test_score[i] = log_loss(y_test_, y_pred)\n",
    "    \n",
    "train_score = np.zeros((clf.n_estimators,), dtype=np.float64)\n",
    "for i, y_pred in enumerate(clf.staged_predict_proba(X_train_)):\n",
    "    train_score[i] = log_loss(y_train_, y_pred)\n",
    "    \n",
    "print(np.argmin(test_score), np.min(test_score))\n",
    "plt.plot(range(len(train_score)), train_score, 'b-',\n",
    "         range(len(test_score)), test_score, 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290 0.538218665554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11d652a58>,\n",
       " <matplotlib.lines.Line2D at 0x11d652cf8>]"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCH0AyAihDwAZ+X+ZqwCwf1YidgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119b74e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X_train_logreg, \n",
    "                                                        y_train,\n",
    "                                                        test_size=0.33, \n",
    "                                                        random_state=54)\n",
    "\n",
    "LEAF = 24 \n",
    "# 550 8 0.540\n",
    "# 80  32 0.539426191726\n",
    "# 110 24 0.53932580292\n",
    "\n",
    "clf = RandomForestClassifier(min_samples_leaf=LEAF,\n",
    "                             warm_start=True,\n",
    "                             n_jobs=3)\n",
    "\n",
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "MIN_ESTIMATORS = 100\n",
    "MAX_ESTIMATORS = 300\n",
    "\n",
    "RANGE = list(range(MIN_ESTIMATORS,MAX_ESTIMATORS,5))\n",
    "\n",
    "for i in RANGE:\n",
    "    clf.set_params(n_estimators=i)\n",
    "    clf.fit(X_train_, y_train_)\n",
    "    y_test_pred = clf.predict_proba(X_test_)\n",
    "    y_train_pred = clf.predict_proba(X_train_)\n",
    "    test_score.append(log_loss(y_test_, y_test_pred))\n",
    "    train_score.append(log_loss(y_train_, y_train_pred))\n",
    "\n",
    "print(RANGE[np.argmin(test_score)], np.min(test_score))\n",
    "plt.plot(RANGE, train_score, 'b-',\n",
    "         RANGE, test_score, 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stacked = np.column_stack((X_train_, X_train_proba_logreg, X_train_proba_gb, X_train_proba_rf))\n",
    "X_test_stacked = np.column_stack((X_test_, X_test_proba_logreg, X_test_proba_gb, X_test_proba_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.765978</td>\n",
       "      <td>0.006132</td>\n",
       "      <td>-0.541263</td>\n",
       "      <td>-0.540907</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'penalty': 'l1', 'C': 1}</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.542202</td>\n",
       "      <td>-0.540566</td>\n",
       "      <td>-0.541632</td>\n",
       "      <td>-0.540783</td>\n",
       "      <td>-0.540614</td>\n",
       "      <td>-0.541157</td>\n",
       "      <td>-0.540605</td>\n",
       "      <td>-0.541122</td>\n",
       "      <td>4.880253</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.941007</td>\n",
       "      <td>0.006024</td>\n",
       "      <td>-0.541264</td>\n",
       "      <td>-0.540906</td>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'penalty': 'l1', 'C': 10}</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.542201</td>\n",
       "      <td>-0.540564</td>\n",
       "      <td>-0.541635</td>\n",
       "      <td>-0.540783</td>\n",
       "      <td>-0.540624</td>\n",
       "      <td>-0.541156</td>\n",
       "      <td>-0.540595</td>\n",
       "      <td>-0.541121</td>\n",
       "      <td>0.102382</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.671370</td>\n",
       "      <td>0.006508</td>\n",
       "      <td>-0.541263</td>\n",
       "      <td>-0.540906</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'penalty': 'l1', 'C': 100}</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.542202</td>\n",
       "      <td>-0.540564</td>\n",
       "      <td>-0.541634</td>\n",
       "      <td>-0.540783</td>\n",
       "      <td>-0.540625</td>\n",
       "      <td>-0.541156</td>\n",
       "      <td>-0.540592</td>\n",
       "      <td>-0.541121</td>\n",
       "      <td>0.059650</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.485851</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>-0.541263</td>\n",
       "      <td>-0.540906</td>\n",
       "      <td>1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'penalty': 'l1', 'C': 1000}</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.542195</td>\n",
       "      <td>-0.540564</td>\n",
       "      <td>-0.541635</td>\n",
       "      <td>-0.540783</td>\n",
       "      <td>-0.540625</td>\n",
       "      <td>-0.541156</td>\n",
       "      <td>-0.540596</td>\n",
       "      <td>-0.541122</td>\n",
       "      <td>0.619989</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_C  \\\n",
       "0      12.765978         0.006132        -0.541263         -0.540907       1   \n",
       "1       3.941007         0.006024        -0.541264         -0.540906      10   \n",
       "2       3.671370         0.006508        -0.541263         -0.540906     100   \n",
       "3       4.485851         0.006028        -0.541263         -0.540906    1000   \n",
       "\n",
       "  param_penalty                        params  rank_test_score  \\\n",
       "0            l1     {'penalty': 'l1', 'C': 1}                3   \n",
       "1            l1    {'penalty': 'l1', 'C': 10}                4   \n",
       "2            l1   {'penalty': 'l1', 'C': 100}                2   \n",
       "3            l1  {'penalty': 'l1', 'C': 1000}                1   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0          -0.542202           -0.540566          -0.541632   \n",
       "1          -0.542201           -0.540564          -0.541635   \n",
       "2          -0.542202           -0.540564          -0.541634   \n",
       "3          -0.542195           -0.540564          -0.541635   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  \\\n",
       "0           -0.540783          -0.540614           -0.541157   \n",
       "1           -0.540783          -0.540624           -0.541156   \n",
       "2           -0.540783          -0.540625           -0.541156   \n",
       "3           -0.540783          -0.540625           -0.541156   \n",
       "\n",
       "   split3_test_score  split3_train_score  std_fit_time  std_score_time  \\\n",
       "0          -0.540605           -0.541122      4.880253        0.000165   \n",
       "1          -0.540595           -0.541121      0.102382        0.000127   \n",
       "2          -0.540592           -0.541121      0.059650        0.000690   \n",
       "3          -0.540596           -0.541122      0.619989        0.000117   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.000684         0.000245  \n",
       "1        0.000684         0.000245  \n",
       "2        0.000685         0.000245  \n",
       "3        0.000682         0.000246  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scoring = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "kfold = KFold(n_splits=4, shuffle=True, random_state=34512)\n",
    "param_grid = {'C': [1, 10, 100, 1000], 'penalty': ['l1']}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=500,\n",
    "                                       class_weight='balanced'), \n",
    "                                       param_grid,\n",
    "                                       scoring,\n",
    "                                       cv=kfold)\n",
    "\n",
    "grid.fit(X_train_stacked, y_train)\n",
    "\n",
    "y_test = grid.best_estimator_.predict_proba(X_test_stacked)\n",
    "y_test = y_test[:,1]\n",
    "\n",
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12129608,  0.01229814,  0.00650047, -0.54793423,  0.30817519,\n",
       "        -0.0400772 , -0.04299587, -0.05812896, -0.23438704, -0.25532507,\n",
       "        -0.8020991 , -0.66647127, -0.46004472, -0.20066617, -0.21177939,\n",
       "        -0.38363658, -0.04949381,  0.52370243, -0.18626931, -1.84612491,\n",
       "         3.37551576,  1.80243958]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.672992\tvalidation_1-logloss:0.673069\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-logloss:0.655572\tvalidation_1-logloss:0.65568\n",
      "[2]\tvalidation_0-logloss:0.640406\tvalidation_1-logloss:0.640604\n",
      "[3]\tvalidation_0-logloss:0.627443\tvalidation_1-logloss:0.627713\n",
      "[4]\tvalidation_0-logloss:0.616236\tvalidation_1-logloss:0.616572\n",
      "[5]\tvalidation_0-logloss:0.606451\tvalidation_1-logloss:0.606892\n",
      "[6]\tvalidation_0-logloss:0.597835\tvalidation_1-logloss:0.598417\n",
      "[7]\tvalidation_0-logloss:0.590266\tvalidation_1-logloss:0.590935\n",
      "[8]\tvalidation_0-logloss:0.583715\tvalidation_1-logloss:0.584508\n",
      "[9]\tvalidation_0-logloss:0.577809\tvalidation_1-logloss:0.57874\n",
      "[10]\tvalidation_0-logloss:0.57259\tvalidation_1-logloss:0.573633\n",
      "[11]\tvalidation_0-logloss:0.568131\tvalidation_1-logloss:0.569298\n",
      "[12]\tvalidation_0-logloss:0.564192\tvalidation_1-logloss:0.565473\n",
      "[13]\tvalidation_0-logloss:0.560653\tvalidation_1-logloss:0.562111\n",
      "[14]\tvalidation_0-logloss:0.557543\tvalidation_1-logloss:0.5591\n",
      "[15]\tvalidation_0-logloss:0.554897\tvalidation_1-logloss:0.556541\n",
      "[16]\tvalidation_0-logloss:0.552537\tvalidation_1-logloss:0.554291\n",
      "[17]\tvalidation_0-logloss:0.550447\tvalidation_1-logloss:0.552291\n",
      "[18]\tvalidation_0-logloss:0.548488\tvalidation_1-logloss:0.550473\n",
      "[19]\tvalidation_0-logloss:0.546863\tvalidation_1-logloss:0.548903\n",
      "[20]\tvalidation_0-logloss:0.545399\tvalidation_1-logloss:0.547552\n",
      "[21]\tvalidation_0-logloss:0.544061\tvalidation_1-logloss:0.546398\n",
      "[22]\tvalidation_0-logloss:0.542932\tvalidation_1-logloss:0.545364\n",
      "[23]\tvalidation_0-logloss:0.541929\tvalidation_1-logloss:0.544486\n",
      "[24]\tvalidation_0-logloss:0.541013\tvalidation_1-logloss:0.543674\n",
      "[25]\tvalidation_0-logloss:0.540226\tvalidation_1-logloss:0.542974\n",
      "[26]\tvalidation_0-logloss:0.539476\tvalidation_1-logloss:0.542405\n",
      "[27]\tvalidation_0-logloss:0.538803\tvalidation_1-logloss:0.541828\n",
      "[28]\tvalidation_0-logloss:0.538225\tvalidation_1-logloss:0.541369\n",
      "[29]\tvalidation_0-logloss:0.537694\tvalidation_1-logloss:0.540986\n",
      "[30]\tvalidation_0-logloss:0.537248\tvalidation_1-logloss:0.540648\n",
      "[31]\tvalidation_0-logloss:0.536802\tvalidation_1-logloss:0.540373\n",
      "[32]\tvalidation_0-logloss:0.536381\tvalidation_1-logloss:0.540108\n",
      "[33]\tvalidation_0-logloss:0.536058\tvalidation_1-logloss:0.539895\n",
      "[34]\tvalidation_0-logloss:0.535718\tvalidation_1-logloss:0.539696\n",
      "[35]\tvalidation_0-logloss:0.535376\tvalidation_1-logloss:0.539538\n",
      "[36]\tvalidation_0-logloss:0.535078\tvalidation_1-logloss:0.539367\n",
      "[37]\tvalidation_0-logloss:0.534777\tvalidation_1-logloss:0.539211\n",
      "[38]\tvalidation_0-logloss:0.534521\tvalidation_1-logloss:0.539085\n",
      "[39]\tvalidation_0-logloss:0.534289\tvalidation_1-logloss:0.538991\n",
      "[40]\tvalidation_0-logloss:0.534109\tvalidation_1-logloss:0.5389\n",
      "[41]\tvalidation_0-logloss:0.533915\tvalidation_1-logloss:0.538915\n",
      "[42]\tvalidation_0-logloss:0.53372\tvalidation_1-logloss:0.538829\n",
      "[43]\tvalidation_0-logloss:0.533497\tvalidation_1-logloss:0.538787\n",
      "[44]\tvalidation_0-logloss:0.533265\tvalidation_1-logloss:0.538716\n",
      "[45]\tvalidation_0-logloss:0.533115\tvalidation_1-logloss:0.53865\n",
      "[46]\tvalidation_0-logloss:0.532912\tvalidation_1-logloss:0.53861\n",
      "[47]\tvalidation_0-logloss:0.532785\tvalidation_1-logloss:0.53858\n",
      "[48]\tvalidation_0-logloss:0.532638\tvalidation_1-logloss:0.538551\n",
      "[49]\tvalidation_0-logloss:0.532486\tvalidation_1-logloss:0.538542\n",
      "[50]\tvalidation_0-logloss:0.532351\tvalidation_1-logloss:0.538527\n",
      "[51]\tvalidation_0-logloss:0.532264\tvalidation_1-logloss:0.538519\n",
      "[52]\tvalidation_0-logloss:0.532141\tvalidation_1-logloss:0.538522\n",
      "[53]\tvalidation_0-logloss:0.531977\tvalidation_1-logloss:0.538477\n",
      "[54]\tvalidation_0-logloss:0.531918\tvalidation_1-logloss:0.538473\n",
      "[55]\tvalidation_0-logloss:0.531847\tvalidation_1-logloss:0.538478\n",
      "[56]\tvalidation_0-logloss:0.531781\tvalidation_1-logloss:0.538502\n",
      "[57]\tvalidation_0-logloss:0.531686\tvalidation_1-logloss:0.538487\n",
      "[58]\tvalidation_0-logloss:0.531533\tvalidation_1-logloss:0.538442\n",
      "[59]\tvalidation_0-logloss:0.531449\tvalidation_1-logloss:0.538447\n",
      "[60]\tvalidation_0-logloss:0.531336\tvalidation_1-logloss:0.538449\n",
      "[61]\tvalidation_0-logloss:0.5312\tvalidation_1-logloss:0.538503\n",
      "[62]\tvalidation_0-logloss:0.531153\tvalidation_1-logloss:0.538537\n",
      "[63]\tvalidation_0-logloss:0.530998\tvalidation_1-logloss:0.538551\n",
      "[64]\tvalidation_0-logloss:0.530887\tvalidation_1-logloss:0.538585\n",
      "[65]\tvalidation_0-logloss:0.530838\tvalidation_1-logloss:0.538568\n",
      "[66]\tvalidation_0-logloss:0.530782\tvalidation_1-logloss:0.538594\n",
      "[67]\tvalidation_0-logloss:0.53061\tvalidation_1-logloss:0.538646\n",
      "[68]\tvalidation_0-logloss:0.530446\tvalidation_1-logloss:0.538618\n",
      "Stopping. Best iteration:\n",
      "[58]\tvalidation_0-logloss:0.531533\tvalidation_1-logloss:0.538442\n",
      "\n",
      "0.530446360106\n",
      "0.538617936387\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/vmurashkin/Projects/netology/000-sklearn/xgboost/python-package')\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0.537957597018\n",
    "# 0.539590006401\n",
    "\n",
    "X_train_, X_test_, y_train_, y_test_, W_train_, W_test_ = train_test_split(X_train_stacked, \n",
    "                                                        y_train,\n",
    "                                                        X_weight,\n",
    "                                                        test_size=0.33, \n",
    "                                                        random_state=43285192)\n",
    "\n",
    "LERANING_RATE = 0.075\n",
    "MIN_CHILD_WEIGHT = 64\n",
    "MAX_DEPTH = 6\n",
    "SUBSAMPLE = 0.75\n",
    "BY_TREE = 0.8\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=1000,\n",
    "                    learning_rate=LERANING_RATE, \n",
    "                    min_child_weight=MIN_CHILD_WEIGHT,\n",
    "                    max_depth=MAX_DEPTH,\n",
    "                    subsample=SUBSAMPLE, \n",
    "                    colsample_bytree=BY_TREE,\n",
    "                    silent=True)\n",
    "\n",
    "xgb.fit(X_train_, y_train_,\n",
    "        eval_set=[(X_train_, y_train_), (X_test_, y_test_),],\n",
    "        eval_metric='logloss', \n",
    "        early_stopping_rounds=10)\n",
    "\n",
    "print(log_loss(y_train_, xgb.predict_proba(X_train_)))\n",
    "print(log_loss(y_test_, xgb.predict_proba(X_test_)))\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=xgb.best_iteration,\n",
    "                    learning_rate=LERANING_RATE, \n",
    "                    min_child_weight=MIN_CHILD_WEIGHT,\n",
    "                    max_depth=MAX_DEPTH,\n",
    "                    subsample=SUBSAMPLE, \n",
    "                    colsample_bytree=BY_TREE,\n",
    "                    silent=True)\n",
    "\n",
    "\n",
    "xgb.fit(X_train_logreg, y_train)\n",
    "\n",
    "X_test_proba_xgb = xgb.predict_proba(X_test_logreg)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = np.power(X_test_proba_gb * X_test_proba_rf * X_test_proba_xgb, 1/3)\n",
    "y_test = X_test_proba_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('submission.txt', 'w') as dst:\n",
    "    dst.writelines('%s\\n' % y for y in y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.478741260568\r\n",
      "0.448059461318\r\n",
      "0.412108690227\r\n",
      "0.530594763693\r\n",
      "0.20395766263\r\n",
      "0.245101739313\r\n",
      "0.189566917555\r\n",
      "0.378119487879\r\n",
      "0.251552237144\r\n",
      "0.894173693324\r\n"
     ]
    }
   ],
   "source": [
    "!head ./submission.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.497684635242\r\n",
      "0.573637497798\r\n",
      "0.372767200332\r\n",
      "0.55355291323\r\n",
      "0.227121019345\r\n",
      "0.252056829349\r\n",
      "0.187007340157\r\n",
      "0.374298622103\r\n",
      "0.27347270647\r\n",
      "0.869167034374\r\n"
     ]
    }
   ],
   "source": [
    "!head /Users/vmurashkin/Downloads/submission.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
